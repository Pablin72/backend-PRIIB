{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizamos todas las importaciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import snowballstemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Primero vamos a transformar todos nuestros archivos con la extension .txt para poder manipularlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed file format\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the folder you want to process\n",
    "src_folder = 'reuters/training'\n",
    "# Specify the path to the destination folder\n",
    "dest_folder = 'reuters/training_txt'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over all the files in the specified folder\n",
    "for filename in os.listdir(src_folder):\n",
    "    # Get the full path of the file\n",
    "    src_file_path = os.path.join(src_folder, filename)\n",
    "    # Define the destination file path with .txt extension\n",
    "    dest_file_path = os.path.join(dest_folder, f\"{filename}.txt\")\n",
    "    # Copy the file to the destination folder with .txt extension\n",
    "    shutil.copy(src_file_path, dest_file_path)\n",
    "print(\"Fixed file format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ahora vamos a eliminar las stop words de nuestra carpeta ya con la extension correcta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words removal and file saving completed.\n"
     ]
    }
   ],
   "source": [
    "# Load stop words from the stop words file\n",
    "stop_words_file = 'reuters/stopwords.txt'\n",
    "with open(stop_words_file, 'r', encoding='utf-8') as file:\n",
    "    stop_words = set(file.read().split())\n",
    "\n",
    "# Specify the path to the source folder and the destination folder\n",
    "src_folder = 'reuters/training_txt'\n",
    "dest_folder = 'reuters/training_stop_words_txt'\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "# Function to read file with different encodings\n",
    "def read_file_with_encodings(file_path):\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1']\n",
    "    for encoding in encodings:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                return file.read()\n",
    "\n",
    "# Iterate over all the files in the specified folder\n",
    "for filename in os.listdir(src_folder):\n",
    "    # Get the full path of the source file\n",
    "    src_file_path = os.path.join(src_folder, filename)\n",
    "    \n",
    "    # Read the content of the file using the function\n",
    "    content = read_file_with_encodings(src_file_path)\n",
    "        \n",
    "    # Remove stop words from the content\n",
    "    cleaned_content = ' '.join([word for word in content.split() if word.lower() not in stop_words])\n",
    "        \n",
    "    # Define the destination file path\n",
    "    dest_file_path = os.path.join(dest_folder, filename)\n",
    "        \n",
    "    # Write the cleaned content to the destination file\n",
    "    with open(dest_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_content)\n",
    "        \n",
    "print(\"Stop words removal and file saving completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. El siguiente paso es eliminar los caracteres especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed and cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "input_directory = 'reuters/training_stop_words_txt'\n",
    "output_directory = 'reuters/final_txt'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove special characters using regex\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Process each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        cleaned_content = clean_text(content)\n",
    "    \n",
    "    # Write the cleaned content to a new file in the output directory\n",
    "    with open(os.path.join(output_directory, filename), 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(cleaned_content)\n",
    "\n",
    "print(\"All files have been processed and cleaned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stematizar y lematizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de lenguaje de spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Inicializar el stemmer\n",
    "stemmer = snowballstemmer.stemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# Función para lematizar y stematizar texto\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    stemmed_tokens = [stemmer.stemWord(token.text) for token in doc]\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(stemmed_tokens), ' '.join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "# Directorios de entrada y salida\n",
    "input_dir = 'reuters/final_txt'\n",
    "output_dir_stemmed = 'final/Stemmed'\n",
    "output_dir_lemmatized = 'final/lemmatized'\n",
    "\n",
    "# Crear directorios de salida si no existen\n",
    "os.makedirs(output_dir_stemmed, exist_ok=True)\n",
    "os.makedirs(output_dir_lemmatized, exist_ok=True)\n",
    "\n",
    "# Procesar cada archivo en el directorio de entrada\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(input_dir, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            stemmed_text, lemmatized_text = preprocess_text(text)\n",
    "        \n",
    "        # Guardar texto stematizado\n",
    "        with open(os.path.join(output_dir_stemmed, filename), 'w', encoding='utf-8') as file:\n",
    "            file.write(stemmed_text)\n",
    "        \n",
    "        # Guardar texto lematizado\n",
    "        with open(os.path.join(output_dir_lemmatized, filename), 'w', encoding='utf-8') as file:\n",
    "            file.write(lemmatized_text)\n",
    "\n",
    "print(\"Procesamiento completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leer archivos limpios\n",
    "\n",
    "Ahora que ya tenemos las dos carpetas limpias vamos a leer cada una y guardar para poder hacer BoW y TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Carpeta de Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos leídos: 7769\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Directorio de la carpeta que contiene los archivos .txt\n",
    "src_folder = 'final/lemmatized'\n",
    "\n",
    "# Función para leer archivos con diferentes codificaciones\n",
    "def read_file_with_encodings(file_path):\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                return file.read()\n",
    "        except:\n",
    "            continue\n",
    "    raise ValueError(f\"Unable to read the file {file_path} with available encodings.\")\n",
    "\n",
    "# Leer todos los archivos .txt en la carpeta\n",
    "documents_lemmatized = []\n",
    "filenames_lemmatized = []\n",
    "for filename in os.listdir(src_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(src_folder, filename)\n",
    "        content = read_file_with_encodings(file_path)\n",
    "        documents_lemmatized.append(content)\n",
    "        filenames_lemmatized.append(filename)\n",
    "\n",
    "# Verificar que se hayan leído los archivos\n",
    "print(f\"Archivos leídos: {len(documents_lemmatized)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Carpeta de Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos leídos: 7769\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Directorio de la carpeta que contiene los archivos .txt\n",
    "src_folder = 'final/Stemmed'\n",
    "\n",
    "# Función para leer archivos con diferentes codificaciones\n",
    "def read_file_with_encodings(file_path):\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                return file.read()\n",
    "        except:\n",
    "            continue\n",
    "    raise ValueError(f\"Unable to read the file {file_path} with available encodings.\")\n",
    "\n",
    "# Leer todos los archivos .txt en la carpeta\n",
    "documents_stemmed = []\n",
    "filenames_stemmed = []\n",
    "for filename in os.listdir(src_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(src_folder, filename)\n",
    "        content = read_file_with_encodings(file_path)\n",
    "        documents_stemmed.append(content)\n",
    "        filenames_stemmed.append(filename)\n",
    "\n",
    "# Verificar que se hayan leído los archivos\n",
    "print(f\"Archivos leídos: {len(documents_stemmed)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Vectorización utilizando Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para vectorizar textos utilizando Bag of Words y TF-IDF\n",
    "def Bag_of_Words(texts):\n",
    "    # Vectorización usando Bag of Words\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(texts)\n",
    "\n",
    "    return X_bow, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Utilizando la carpeta Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Lemmatized:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Caracteristicas de BoW Lemmatized: ['000' '0006913' '0006916' ... 'zuyuan' 'zverev' 'zzzz']\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar los documentos\n",
    "X_bow_lemmatized, bow_vectorizer_lemmatized = Bag_of_Words(documents_lemmatized)\n",
    "\n",
    "# Ver los resultados de Bag of Words\n",
    "print(\"Bag of Words (BoW) Lemmatized:\")\n",
    "print(X_bow_lemmatized.toarray())\n",
    "print(\"Caracteristicas de BoW Lemmatized:\", bow_vectorizer_lemmatized.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Utilizando la carpeta Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Stemmed:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Caracteristicas de BoW Stemmed: ['000' '0006913' '0006916' ... 'zuyuan' 'zverev' 'zzzz']\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar los documentos\n",
    "X_bow_stemmed, bow_vectorizer_stemmed = Bag_of_Words(documents_stemmed)\n",
    "\n",
    "# Ver los resultados de Bag of Words\n",
    "print(\"Bag of Words (BoW) Stemmed:\")\n",
    "print(X_bow_stemmed.toarray())\n",
    "print(\"Caracteristicas de BoW Stemmed:\", bow_vectorizer_stemmed.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Vectorización utilizando TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para vectorizar textos utilizando Bag of Words y TF-IDF\n",
    "def TF_IDF(texts):\n",
    "    # Vectorización usando TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    return X_tfidf, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Utilizando la carpeta Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Lemmatized:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Caracteristicas de TF-IDF Lemmatized: ['000' '0006913' '0006916' ... 'zuyuan' 'zverev' 'zzzz']\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar los documentos\n",
    "X_tfidf_lemmatized, tfidf_vectorizer_lemmatized = TF_IDF(documents_lemmatized)\n",
    "\n",
    "# Ver los resultados de TF-IDF\n",
    "print(\"TF-IDF Lemmatized:\")\n",
    "print(X_tfidf_lemmatized.toarray())\n",
    "print(\"Caracteristicas de TF-IDF Lemmatized:\", tfidf_vectorizer_lemmatized.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Utilizando la carpeta Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Stemmed:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Caracteristicas de TF-IDF Stemmed: ['000' '0006913' '0006916' ... 'zuyuan' 'zverev' 'zzzz']\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar los documentos\n",
    "X_tfidf_Stemmed, tfidf_vectorizer_Stemmed = TF_IDF(documents_stemmed)\n",
    "\n",
    "# Ver los resultados de TF-IDF\n",
    "print(\"TF-IDF Stemmed:\")\n",
    "print(X_tfidf_Stemmed.toarray())\n",
    "print(\"Caracteristicas de TF-IDF Stemmed:\", tfidf_vectorizer_Stemmed.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ahora todo vamos a transformar a 4 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bag of Word\n",
    "df_bow_lemmatized = pd.DataFrame(X_bow_lemmatized.toarray(), columns=bow_vectorizer_lemmatized.get_feature_names_out(), index=filenames_lemmatized)\n",
    "df_bow_stemmed = pd.DataFrame(X_bow_stemmed.toarray(), columns=bow_vectorizer_stemmed.get_feature_names_out(), index=filenames_stemmed)\n",
    "\n",
    "# TF_IDF\n",
    "df_tfidf_lemmatized = pd.DataFrame(X_tfidf_lemmatized.toarray(), columns=tfidf_vectorizer_lemmatized.get_feature_names_out(), index=filenames_lemmatized)\n",
    "df_tfidf_stemmed = pd.DataFrame(X_tfidf_Stemmed.toarray(), columns=tfidf_vectorizer_Stemmed.get_feature_names_out(), index=filenames_stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(documents_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2\n",
      "  (0, 166)\t1\n",
      "  (0, 4788)\t1\n",
      "  (0, 14852)\t1\n",
      "  (0, 15793)\t1\n",
      "  (0, 16453)\t2\n",
      "  (0, 18279)\t1\n",
      "  (0, 18357)\t1\n",
      "  (0, 19659)\t1\n",
      "  (0, 21682)\t1\n",
      "  (0, 22413)\t1\n",
      "  (0, 22432)\t1\n",
      "  (0, 23768)\t1\n",
      "  (0, 24697)\t1\n",
      "  (0, 28442)\t2\n",
      "  (0, 29268)\t2\n",
      "  (0, 30667)\t1\n",
      "  (0, 31444)\t2\n",
      "  (0, 33323)\t1\n",
      "  (0, 33772)\t1\n",
      "  (0, 36133)\t1\n",
      "  (0, 36192)\t1\n",
      "  (0, 38123)\t1\n"
     ]
    }
   ],
   "source": [
    "query = \"NIPPON MINING LOWERS COPPER PRICE Nippon Mining Co Ltd said it lowered its selling price for electrolytic copper by 10,000 yen per tonne to 260,000, effective immediately.\"\n",
    "\n",
    "query_vector = vectorizer_bow.transform([query])\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Jaccard con Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Para la carpete Lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7769, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calcular coeficiente de Jaccard entre los conjuntos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m jaccard_coefficient_lemmatized \u001b[38;5;241m=\u001b[39m \u001b[43mjaccard_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_bow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_vector\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EPN/Sexto-Semestre/Information-Retrieval/PROYECTO/BACKEND/backend-PRRIB/env/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/EPN/Sexto-Semestre/Information-Retrieval/PROYECTO/BACKEND/backend-PRRIB/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:851\u001b[0m, in \u001b[0;36mjaccard_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    702\u001b[0m     {\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    727\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    728\u001b[0m ):\n\u001b[1;32m    729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Jaccard similarity coefficient score.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m    The Jaccard index [1], or Jaccard similarity coefficient, defined as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    array([1. , 0. , 0.33...])\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 851\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m     samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    853\u001b[0m     MCM \u001b[38;5;241m=\u001b[39m multilabel_confusion_matrix(\n\u001b[1;32m    854\u001b[0m         y_true,\n\u001b[1;32m    855\u001b[0m         y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    858\u001b[0m         samplewise\u001b[38;5;241m=\u001b[39msamplewise,\n\u001b[1;32m    859\u001b[0m     )\n",
      "File \u001b[0;32m~/EPN/Sexto-Semestre/Information-Retrieval/PROYECTO/BACKEND/backend-PRRIB/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1501\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1501\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/EPN/Sexto-Semestre/Information-Retrieval/PROYECTO/BACKEND/backend-PRRIB/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/EPN/Sexto-Semestre/Information-Retrieval/PROYECTO/BACKEND/backend-PRRIB/env/lib/python3.8/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7769, 1]"
     ]
    }
   ],
   "source": [
    "# Calcular coeficiente de Jaccard entre los conjuntos\n",
    "jaccard_coefficient_lemmatized = jaccard_score(X_bow, query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Creamos el Bag of Words con CountVectorizer de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read all .txt files from the directory\n",
    "directory = 'reuters/final_txt'\n",
    "\n",
    "filenames = []\n",
    "all_sentences = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        all_sentences.append(content)\n",
    "        filenames.append(filename)\n",
    "\n",
    "\n",
    "print(all_sentences)\n",
    "\n",
    "# Step 3: Use CountVectorizer to vectorize the text data\n",
    "# Each row represents a document.\n",
    "# Each column represents a unique token (word) from the corpus.\n",
    "# Each entry in the matrix represents the count of the token in the corresponding document.\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(all_sentences)\n",
    "\n",
    "terms_bow = vectorizer_bow.get_feature_names_out()\n",
    "\n",
    "#\n",
    "# print(all_sentences)\n",
    "\n",
    "# Step 4: Print the resulting bag-of-words array\n",
    "X_bow = X_bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity based on the BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate cosine similarity scores\n",
    "cosine_sim_scores = cosine_similarity(X_bow, query_vector)\n",
    "\n",
    "# Create a DataFrame to store document filenames and their similarity scores\n",
    "similarity_df = pd.DataFrame({'Filename': filenames, 'Cosine_Similarity': cosine_sim_scores.flatten()})\n",
    "\n",
    "# Sort documents based on similarity scores\n",
    "similarity_df = similarity_df.sort_values(by='Cosine_Similarity', ascending=False)\n",
    "\n",
    "# Display the ranked documents\n",
    "print(similarity_df)\n",
    "\n",
    "\n",
    "# Print the content of the top three ranked documents\n",
    "top_ten_filenames = similarity_df.head(10)['Filename'].tolist()\n",
    "\n",
    "original_directory = 'reuters/test_txt'\n",
    "\n",
    "\n",
    "for filename in top_ten_filenames:\n",
    "    with open(os.path.join(original_directory, filename), 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        print(f\"Document: {filename}\")\n",
    "        print(content)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vectorizamos los textos usando TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row represents a document.\n",
    "# Each column represents a unique token (word) from the corpus.\n",
    "# Each entry in the matrix represents the TF-IDF score of the token in the corresponding document.\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(all_sentences)\n",
    "\n",
    "\n",
    "# Print the resulting TF-IDF array\n",
    "print(X_tfidf.toarray())\n",
    "\n",
    "\n",
    "# Get feature names (terms)\n",
    "terms = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for readability\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), index=filenames, columns=terms)\n",
    "\n",
    "# Display the TF-IDF array\n",
    "print(\"TF-IDF Array:\")\n",
    "\n",
    "tfidf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
